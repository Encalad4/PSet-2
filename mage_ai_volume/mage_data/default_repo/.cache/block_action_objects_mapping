{"block_file": {"custom/create_partitioned_payment_type.sql:custom:sql:create partitioned payment type": {"content": "-- Docs: https://docs.mage.ai/guides/sql-blocks\nDROP TABLE IF EXISTS analytics_gold.dim_payment_type CASCADE;\n\nCREATE TABLE gold.dim_payment_type (\n    payment_type_key  INTEGER NOT NULL PRIMARY KEY,\n    payment_type_name TEXT    NOT NULL,\n    partition_group   TEXT    NOT NULL\n) PARTITION BY LIST (partition_group);\n\n-- Crear particiones por grupo\nCREATE TABLE gold.dim_payment_type_cash\n    PARTITION OF gold.dim_payment_type\n    FOR VALUES IN ('cash');\n\nCREATE TABLE gold.dim_payment_type_card\n    PARTITION OF gold.dim_payment_type\n    FOR VALUES IN ('card');\n\nCREATE TABLE gold.dim_payment_type_flex\n    PARTITION OF gold.dim_payment_type\n    FOR VALUES IN ('flex');\n\nCREATE TABLE gold.dim_payment_type_other\n    PARTITION OF gold.dim_payment_type\n    FOR VALUES IN ('other/unknown');", "file_path": "custom/create_partitioned_payment_type.sql", "language": "sql", "type": "custom", "uuid": "create_partitioned_payment_type"}, "custom/run_silver_pipeline.py:custom:python:run silver pipeline": {"content": "if 'custom' not in globals():\n    from mage_ai.data_preparation.decorators import custom\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\nfrom mage_ai.orchestration.triggers.api import trigger_pipeline\n\n@custom\ndef trigger_dbt_pipeline(*args, **kwargs):\n    \"\"\"\n    Trigger the dbt_build_silver pipeline after ingest_bronze completes\n    \"\"\"\n    \n    trigger_pipeline(\n        'dbt_build_silver', \n        check_status=False,    \n        error_on_failure=False, \n        verbose=True,\n        schedule_name='run_after_ingest_bronze',         \n    )\n    \n    return {\"status\": \"dbt_build_silver triggered successfully\"}\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n    assert 'status' in output, 'Status not found in output'", "file_path": "custom/run_silver_pipeline.py", "language": "python", "type": "custom", "uuid": "run_silver_pipeline"}, "custom/start_dbt_build_gold.py:custom:python:start dbt build gold": {"content": "if 'custom' not in globals():\n    from mage_ai.data_preparation.decorators import custom\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\nfrom mage_ai.orchestration.triggers.api import trigger_pipeline\n\n@custom\ndef trigger_dbt_pipeline(*args, **kwargs):\n    \"\"\"\n    Trigger the dbt_build_gold pipeline after dbt_build_silver completes\n    \"\"\"\n    \n    trigger_pipeline(\n        'dbt_build_gold', \n        check_status=False,    \n        error_on_failure=False, \n        verbose=True,\n        schedule_name='run_after_dbt_build_silver',         \n    )\n    \n    return {\"status\": \"dbt_build_gold triggered successfully\"}\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n    assert 'status' in output, 'Status not found in output'", "file_path": "custom/start_dbt_build_gold.py", "language": "python", "type": "custom", "uuid": "start_dbt_build_gold"}, "data_exporters/creating_partitioning_dim_payment_type.py:data_exporter:python:creating partitioning dim payment type": {"content": "if 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\nfrom sqlalchemy import create_engine, text\nimport gc\n\n@data_exporter\ndef create_partitioned_table(*args, **kwargs):\n    \"\"\"\n    Creates partitioned dim_payment_type table in PostgreSQL using Mage Secrets\n    \"\"\"\n    logger = kwargs.get('logger')\n    \n    # 1. CONEXI\u00d3N DIRECTA\n    db_user = get_secret_value('POSTGRES_USER')\n    db_password = get_secret_value('POSTGRES_PASSWORD')\n    db_host = get_secret_value('POSTGRES_HOST')\n    db_port = '5432'\n    db_name = get_secret_value('POSTGRES_DB')\n    \n    # Crear conexi\u00f3n\n    connection_string = f'postgresql://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}'\n    engine = create_engine(connection_string)\n    \n    schema_name = 'analytics_gold'\n    \n    logger.info(\"Starting creation of partitioned dim_payment_type table\")\n    \n    # 2. CREAR SCHEMA SI NO EXISTE\n    with engine.connect() as conn:\n        with conn.begin():\n            conn.execute(text(f\"CREATE SCHEMA IF NOT EXISTS {schema_name}\"))\n        logger.info(f\"Schema {schema_name} verificado/creado\")\n    \n    # 3. CREAR TABLA PARTICIONADA\n    create_table_sql = f\"\"\"\n    CREATE TABLE IF NOT EXISTS {schema_name}.dim_payment_type (\n        payment_type_key INTEGER,\n        payment_type_name VARCHAR(50),\n        partition_group VARCHAR(20)\n    ) PARTITION BY LIST (partition_group);\n    \n    -- Create partitions\n    CREATE TABLE IF NOT EXISTS {schema_name}.dim_payment_type_cash PARTITION OF {schema_name}.dim_payment_type\n    FOR VALUES IN ('cash');\n    \n    CREATE TABLE IF NOT EXISTS {schema_name}.dim_payment_type_card PARTITION OF {schema_name}.dim_payment_type\n    FOR VALUES IN ('card');\n    \n    CREATE TABLE IF NOT EXISTS {schema_name}.dim_payment_type_flex PARTITION OF {schema_name}.dim_payment_type\n    FOR VALUES IN ('flex');\n    \n    CREATE TABLE IF NOT EXISTS {schema_name}.dim_payment_type_other PARTITION OF {schema_name}.dim_payment_type\n    FOR VALUES IN ('other/unknown');\n    \"\"\"\n    \n    try:\n        with engine.connect() as conn:\n            with conn.begin():\n                conn.execute(text(create_table_sql))\n        logger.info(\"Successfully created partitioned table gold.dim_payment_type\")\n    except Exception as e:\n        logger.error(f\"Error creating partitioned table: {str(e)}\")\n        raise\n    \n    gc.collect()\n    return {\"table_created\": f\"{schema_name}.dim_payment_type\"}", "file_path": "data_exporters/creating_partitioning_dim_payment_type.py", "language": "python", "type": "data_exporter", "uuid": "creating_partitioning_dim_payment_type"}, "data_exporters/executing_backfill.py:data_exporter:python:executing backfill": {"content": "# data_exporter: orchestrate_ingestion/trigger_pipelines.py\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\nfrom mage_ai.orchestration.triggers.api import trigger_pipeline\nimport time\nimport gc\n@data_exporter\ndef trigger_pipelines(pending_df, *args, **kwargs):\n    \"\"\"\n    Ejecuta ingest_bronze para cada combinaci\u00f3n pendiente\n    \"\"\" \n    logger = kwargs.get('logger')\n    \n    if pending_df.empty:\n        logger.info(\"\ud83c\udf89 No hay combinaciones pendientes. Todo completado!\")\n        return {\"status\": \"complete\", \"triggered\": 0}\n    \n    results = {\n        'success': [],\n        'failed': [],\n        'skipped': []\n    } \n    \n    \n    pending_list = pending_df.to_dict('records')\n    \n    logger.info(f\"Iniciando ejecuci\u00f3n de {len(pending_list)} pipelines...\")\n    \n    for i, combo in enumerate(pending_list, 1):\n        year_month = combo['year_month']\n        service_type = combo['service_type']\n        \n        logger.info(f\"[{i}/{len(pending_list)}] Ejecutando: {service_type} - {year_month}\")\n        \n        try:\n           \n            trigger_pipeline(\n                'ingest_bronze', \n                variables={\n                    'year_month': year_month,\n                    'service_type': service_type\n                },\n                check_status=True,  \n                error_on_failure=False, \n                poll_interval=30,  \n                poll_timeout=7200, \n                verbose=True\n            )\n            \n            results['success'].append(f\"{service_type}-{year_month}\")\n            logger.info(f\"Completado: {service_type} - {year_month}\")\n            \n            \n            time.sleep(5)\n            gc.collect()\n        except Exception as e:\n            results['failed'].append(f\"{service_type}-{year_month}\")\n            logger.error(f\"Fall\u00f3: {service_type} - {year_month}: {str(e)}\")\n    \n    \n    logger.info(\"=\"*50)\n    logger.info(\"RESUMEN FINAL:\")\n    logger.info(f\"Exitosos: {len(results['success'])}\")\n    logger.info(f\"Fallidos: {len(results['failed'])}\")\n    logger.info(\"=\"*50)\n    \n    return results", "file_path": "data_exporters/executing_backfill.py", "language": "python", "type": "data_exporter", "uuid": "executing_backfill"}, "data_exporters/export_titanic_clean.py:data_exporter:python:export titanic clean": {"content": "from mage_ai.io.file import FileIO\nfrom pandas import DataFrame\n\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\n\n@data_exporter\ndef export_data_to_file(df: DataFrame, **kwargs) -> None:\n    \"\"\"\n    Template for exporting data to filesystem.\n\n    Docs: https://docs.mage.ai/design/data-loading#example-loading-data-from-a-file\n    \"\"\"\n    filepath = 'titanic_clean.csv'\n    FileIO().export(df, filepath)\n", "file_path": "data_exporters/export_titanic_clean.py", "language": "python", "type": "data_exporter", "uuid": "export_titanic_clean"}, "data_exporters/uploading_trips_to_db.py:data_exporter:python:uploading trips to db": {"content": "if 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\nimport pandas as pd\nfrom sqlalchemy import create_engine, text\nimport gc\n\n@data_exporter\ndef export_data(data, *args, **kwargs):\n    \"\"\"\n    Exporta datos a PostgreSQL con idempotencia:\n    - Usa checkpoint para decidir si eliminar datos existentes\n    - Versi\u00f3n optimizada para memoria usando streaming\n    \"\"\"\n    logger = kwargs.get('logger')\n    \n    \n    year_month = kwargs.get(\"year_month\")\n    service_type = kwargs.get(\"service_type\") \n\n    current_date = datetime.now()\n    current_year_month = current_date.strftime(\"%Y-%m\")\n    \n    if not year_month:\n        year_month = current_year_month\n    if not service_type:\n        service_type = \"yellow\"\n    \n    if not year_month or not service_type:\n        raise ValueError(\"Se requieren year_month y service_type\")\n     \n    \n    logger.info(f\"Procesando datos para {service_type} - {year_month}\")\n    \n    if service_type == 'yellow':\n        logger.info(\"Renombrando columnas de Yellow Taxi (tpep_ -> lpep_)\")\n        rename_map = {\n            'tpep_pickup_datetime': 'lpep_pickup_datetime',\n            'tpep_dropoff_datetime': 'lpep_dropoff_datetime'\n        }\n        existing_renames = {k: v for k, v in rename_map.items() if k in data.columns}\n        if existing_renames:\n            data.rename(columns=existing_renames, inplace=True)\n\n        if 'Airport_fee' in data.columns:\n            data.drop(columns=['Airport_fee'], inplace=True)\n        elif 'airport_fee' in data.columns:\n            data.drop(columns=['airport_fee'], inplace=True)\n\n        \n        missing_cols = ['trip_type', 'ehail_fee']\n        for col in missing_cols:\n            if col not in data.columns:\n                data[col] = None\n            \n    elif service_type == 'green':\n        logger.info(\"Datos de Green Taxi, columnas ya en formato lpep_\")\n    \n   \n    try:\n        db_user = get_secret_value('POSTGRES_USER')\n        db_password = get_secret_value('POSTGRES_PASSWORD')\n        db_host = get_secret_value('POSTGRES_HOST')\n        db_port = '5432'\n        db_name = get_secret_value('POSTGRES_DB')\n        \n        connection_string = f'postgresql://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}'\n        engine = create_engine(connection_string)\n        logger.info(\"\u2713 Conexi\u00f3n a base de datos establecida\")\n    except Exception as e:\n        logger.error(f\"\u274c Error conectando a base de datos: {e}\")\n        raise Exception(f\"Error de conexi\u00f3n a BD: {e}\")\n    \n    schema_name = 'raw' \n    table_name = 'taxi_trips'\n    \n    \n    with engine.connect() as conn:\n        with conn.begin():\n            conn.execute(text(f\"CREATE SCHEMA IF NOT EXISTS {schema_name}\"))\n        logger.info(f\"\u2713 Schema {schema_name} verificado/creado\")\n    \n    \n    with engine.connect() as conn:\n        with conn.begin():\n            dtype_mapping = {\n                'int64': 'INTEGER',\n                'float64': 'FLOAT',\n                'object': 'TEXT',\n                'datetime64[ns]': 'TIMESTAMP',\n                'bool': 'BOOLEAN'\n            }\n            \n            columns = []\n            for col_name, col_type in data.dtypes.items():\n                sql_type = dtype_mapping.get(str(col_type), 'TEXT')\n                columns.append(f'\"{col_name}\" {sql_type}')\n            \n            if 'ingest_ts' not in data.columns:\n                columns.append('\"ingest_ts\" TIMESTAMP')\n            if 'source_month' not in data.columns:\n                columns.append('\"source_month\" TEXT')\n            if 'service_type' not in data.columns:\n                columns.append('\"service_type\" TEXT')\n            \n            create_table_sql = f\"\"\"\n                CREATE TABLE IF NOT EXISTS {schema_name}.{table_name} (\n                    {', '.join(columns)}\n                )\n            \"\"\"\n            conn.execute(text(create_table_sql))\n            logger.info(f\"\u2713 Tabla {schema_name}.{table_name} verificada/creada\")\n    \n    \n    should_delete = False\n    \n   \n    with engine.connect() as conn:\n        result = conn.execute(\n            text(\"\"\"\n                SELECT status, completed_at \n                FROM orchestration.ingestion_checkpoint \n                WHERE year_month = :month AND service_type = :service\n            \"\"\"),\n            {\"month\": year_month, \"service\": service_type}\n        ).first()\n        \n        if result:\n            status = result[0]\n            completed_at = result[1]\n            \n            if status == 'completed':\n                logger.info(f\"\u2713 Checkpoint indica que {service_type}-{year_month} ya fue completado en {completed_at}\")\n                logger.info(f\"  Eliminando datos existentes por idempotencia...\")\n                should_delete = True\n            elif status == 'processing':\n                logger.info(f\"  Checkpoint indica que {service_type}-{year_month} est\u00e1 en procesamiento\")\n                logger.info(f\"  Eliminando datos existentes para comenzar de nuevo...\")\n                should_delete = True\n            elif status == 'failed':\n                logger.info(f\" Checkpoint indica que {service_type}-{year_month} fall\u00f3 anteriormente\")\n                logger.info(f\"  Eliminando datos existentes para reintentar...\")\n                should_delete = True\n        else:\n            logger.info(f\"No hay checkpoint para {service_type}-{year_month}, insertando datos nuevos\")\n   \n    if should_delete:\n        with engine.connect() as conn:\n            with conn.begin():\n                delete_sql = text(f\"\"\"\n                    DELETE FROM {schema_name}.{table_name} \n                    WHERE source_month = :month AND service_type = :service\n                \"\"\")\n                result = conn.execute(delete_sql, {\"month\": year_month, \"service\": service_type})\n            logger.info(f\"\u2713 Eliminados {result.rowcount} registros existentes para {service_type} - {year_month}\")\n    else:\n        logger.info(f\"\u2192 No es necesario eliminar datos existentes\")\n    \n    \n    with engine.connect() as conn:\n        with conn.begin():\n            \n            conn.execute(\n                text(\"\"\"\n                    INSERT INTO orchestration.ingestion_checkpoint \n                    (year_month, service_type, status, started_at, retry_count)\n                    VALUES (:month, :service, 'processing', NOW(), 1)\n                    ON CONFLICT (year_month, service_type) \n                    DO UPDATE SET \n                        status = 'processing',\n                        started_at = NOW(),\n                        retry_count = orchestration.ingestion_checkpoint.retry_count + 1,\n                        error_message = NULL,\n                        updated_at = NOW()\n                \"\"\"),\n                {\n                    \"month\": year_month, \n                    \"service\": service_type\n                }\n            )\n    logger.info(f\"\u2713 Checkpoint actualizado a 'processing' para {service_type}-{year_month}\")\n    \n\n    rows_inserted = 0\n    chunksize = 25000\n    \n    columns = data.columns.tolist()\n    \n    placeholders = ', '.join([f':{col}' for col in columns])\n    column_names = ', '.join([f'\"{col}\"' for col in columns])\n    insert_query = f\"\"\"\n        INSERT INTO {schema_name}.{table_name} ({column_names})\n        VALUES ({placeholders})\n    \"\"\"\n    \n    total_rows = len(data)\n    logger.info(f\"Iniciando inserci\u00f3n de {total_rows} filas en chunks de {chunksize}\")\n    \n    with engine.connect() as conn:\n        with conn.begin():\n            for start_idx in range(0, total_rows, chunksize):\n                end_idx = min(start_idx + chunksize, total_rows)\n                \n                chunk = data.iloc[start_idx:end_idx]\n                \n                chunk_records = []\n                for _, row in chunk.iterrows():\n                    record = {}\n                    for col in columns:\n                        value = row[col]\n                        if pd.isna(value):\n                            record[col] = None\n                        elif isinstance(value, pd.Timestamp):\n                            record[col] = value.isoformat()\n                        else:\n                            record[col] = value\n                    chunk_records.append(record)\n                \n                conn.execute(text(insert_query), chunk_records)\n                \n                rows_inserted += len(chunk)\n                logger.info(f\"Progreso: {rows_inserted}/{total_rows} filas insertadas ({rows_inserted/total_rows*100:.1f}%)\")\n                \n                del chunk_records\n                del chunk\n                gc.collect()\n    \n    logger.info(f\"\u2713 Insertadas {rows_inserted} filas en total\")\n    \n   \n    try:\n        with engine.connect() as conn:\n            with conn.begin():\n                conn.execute(\n                    text(\"\"\"\n                        UPDATE orchestration.ingestion_checkpoint \n                        SET status = 'completed',\n                            completed_at = NOW(),\n                            rows_ingested = :rows,\n                            updated_at = NOW()\n                        WHERE year_month = :month AND service_type = :service\n                    \"\"\"),\n                    {\n                        \"month\": year_month, \n                        \"service\": service_type,\n                        \"rows\": rows_inserted\n                    }\n                )\n        logger.info(f\"Checkpoint actualizado a 'completed'\")\n    except Exception as e:\n        logger.error(f\"Error actualizando checkpoint a 'completed': {e}\")\n        raise\n    \n    gc.collect()\n    return {\"status\": \"success\", \"rows_inserted\": rows_inserted}", "file_path": "data_exporters/uploading_trips_to_db.py", "language": "python", "type": "data_exporter", "uuid": "uploading_trips_to_db"}, "data_exporters/uploading_zones_to_db.py:data_exporter:python:uploading zones to db": {"content": "if 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\n\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\nimport pandas as pd\nfrom sqlalchemy import create_engine, text\nimport os\nimport gc\n@data_exporter\ndef export_data(data, *args, **kwargs):\n    \"\"\"\n    Exporta datos de zonas a PostgreSQL\n    - Reemplaza todos los datos cada vez que se ejecuta\n    \"\"\"\n    logger = kwargs.get('logger')\n    \n    \n    db_user = get_secret_value('POSTGRES_USER')\n    db_password = get_secret_value('POSTGRES_PASSWORD')\n    db_host = get_secret_value('POSTGRES_HOST')\n    db_port = '5432'\n    db_name = get_secret_value('POSTGRES_DB')\n    \n    \n    connection_string = f'postgresql://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}'\n    engine = create_engine(connection_string)\n    \n    schema_name = 'raw'\n    table_name = 'taxi_zones'\n    \n   \n    with engine.connect() as conn:\n        with conn.begin():\n            conn.execute(text(f\"CREATE SCHEMA IF NOT EXISTS {schema_name}\"))\n        logger.info(f\"Schema {schema_name} verificado/creado\")\n    \n    \n    with engine.connect() as conn:\n        with conn.begin():\n            \n            dtype_mapping = {\n                'int64': 'INTEGER',\n                'float64': 'FLOAT',\n                'object': 'TEXT',\n                'datetime64[ns]': 'TIMESTAMP',\n                'bool': 'BOOLEAN'\n            }\n            \n            \n            columns = []\n            for col_name, col_type in data.dtypes.items():\n                sql_type = dtype_mapping.get(str(col_type), 'TEXT')\n                columns.append(f'\"{col_name}\" {sql_type}')\n            \n            \n            create_table_sql = f\"\"\"\n                CREATE TABLE IF NOT EXISTS {schema_name}.{table_name} (\n                    {', '.join(columns)}\n                )\n            \"\"\"\n            conn.execute(text(create_table_sql))\n            logger.info(f\"Tabla {schema_name}.{table_name} verificada/creada\")\n    \n    \n    with engine.connect() as conn:\n        with conn.begin():\n            delete_sql = text(f\"DELETE FROM {schema_name}.{table_name}\")\n            result = conn.execute(delete_sql)\n        logger.info(f\"Eliminados {result.rowcount} registros existentes\")\n    \n    \n    data.to_sql(\n        table_name,\n        con=engine,\n        schema=schema_name,\n        if_exists='append', \n        index=False,\n        chunksize=10000\n    )\n    \n    logger.info(f\"Insertadas {len(data)} filas en {schema_name}.{table_name}\")\n    gc.collect()\n    return {\"status\": \"success\", \"rows_inserted\": len(data)}", "file_path": "data_exporters/uploading_zones_to_db.py", "language": "python", "type": "data_exporter", "uuid": "uploading_zones_to_db"}, "data_loaders/create_checkpoint_table.py:data_loader:python:create checkpoint table": {"content": "# data_loader: create_checkpoint_table.py\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\nfrom sqlalchemy import create_engine, text\nimport pandas as pd\n\n@data_loader\ndef create_checkpoint_table(*args, **kwargs):\n    \"\"\"\n    Crea el esquema orchestration y la tabla de checkpoint si no existen\n    \"\"\"\n    logger = kwargs.get('logger')\n    \n    # 1. Obtener credenciales\n    db_user = get_secret_value('POSTGRES_USER')\n    db_password = get_secret_value('POSTGRES_PASSWORD')\n    db_host = get_secret_value('POSTGRES_HOST')\n    db_port = '5432'\n    db_name = get_secret_value('POSTGRES_DB')\n    \n    # 2. Crear conexi\u00f3n\n    connection_string = f'postgresql://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}'\n    engine = create_engine(connection_string)\n    \n    # 3. Crear esquema orchestration si no existe\n    with engine.connect() as conn:\n        with conn.begin():\n            conn.execute(text(\"CREATE SCHEMA IF NOT EXISTS orchestration\"))\n            logger.info(\"\u2713 Esquema 'orchestration' verificado/creado\")\n    \n    # 4. Crear tabla de checkpoint si no existe\n    create_table_sql = \"\"\"\n    CREATE TABLE IF NOT EXISTS orchestration.ingestion_checkpoint (\n        id SERIAL PRIMARY KEY,\n        year_month VARCHAR(7) NOT NULL,\n        service_type VARCHAR(10) NOT NULL,\n        status VARCHAR(20) NOT NULL DEFAULT 'pending',\n        started_at TIMESTAMP,\n        completed_at TIMESTAMP,\n        rows_ingested INTEGER,\n        error_message TEXT,\n        retry_count INTEGER DEFAULT 0,\n        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n        updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n        UNIQUE(year_month, service_type)\n    );\n    \n    CREATE INDEX IF NOT EXISTS idx_checkpoint_status \n        ON orchestration.ingestion_checkpoint(status);\n    CREATE INDEX IF NOT EXISTS idx_checkpoint_year_month \n        ON orchestration.ingestion_checkpoint(year_month);\n    \"\"\"\n    \n    with engine.connect() as conn:\n        with conn.begin():\n            # Ejecutar cada comando por separado\n            for statement in create_table_sql.split(';'):\n                if statement.strip():\n                    conn.execute(text(statement))\n            logger.info(\"\u2713 Tabla 'ingestion_checkpoint' verificada/creada\")\n    \n    # 5. Retornar un DataFrame vac\u00edo o mensaje de \u00e9xito\n    return pd.DataFrame({'status': ['success'], 'message': ['Checkpoint table ready']})\n\n@test\ndef test_output(output, *args) -> None:\n    assert output is not None, 'The output is undefined'\n    assert output['status'].iloc[0] == 'success', 'Checkpoint creation failed'", "file_path": "data_loaders/create_checkpoint_table.py", "language": "python", "type": "data_loader", "uuid": "create_checkpoint_table"}, "data_loaders/creating_checkpoint_scheme_table.py:data_loader:python:creating checkpoint scheme table": {"content": "# data_loader: orchestrate_ingestion/check_pending.py\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\nfrom sqlalchemy import create_engine, text\nimport pandas as pd\nfrom datetime import datetime\nimport itertools\n\n@data_loader\ndef check_pending(*args, **kwargs):\n    \"\"\"\n    Genera lista de par\u00e1metros pendientes basado en checkpoint\n    \"\"\"\n    logger = kwargs.get('logger')\n    \n    \n    db_user = get_secret_value('POSTGRES_USER')\n    db_password = get_secret_value('POSTGRES_PASSWORD')\n    db_host = get_secret_value('POSTGRES_HOST')\n    db_port = '5432'\n    db_name = get_secret_value('POSTGRES_DB')\n    \n    connection_string = f'postgresql://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}'\n    engine = create_engine(connection_string)\n    \n    \n    with engine.connect() as conn:\n        with conn.begin():\n            \n            conn.execute(text(\"CREATE SCHEMA IF NOT EXISTS orchestration\"))\n            logger.info(\"\u2713 Esquema 'orchestration' verificado\")\n            \n           \n            create_table_sql = \"\"\"\n            CREATE TABLE IF NOT EXISTS orchestration.ingestion_checkpoint (\n                id SERIAL PRIMARY KEY,\n                year_month VARCHAR(7) NOT NULL,\n                service_type VARCHAR(10) NOT NULL,\n                status VARCHAR(20) NOT NULL DEFAULT 'pending',\n                started_at TIMESTAMP,\n                completed_at TIMESTAMP,\n                rows_ingested INTEGER,\n                error_message TEXT,\n                retry_count INTEGER DEFAULT 0,\n                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n                UNIQUE(year_month, service_type) \n            );\n            \"\"\"\n            conn.execute(text(create_table_sql))\n            logger.info(\"\u2713 Tabla 'ingestion_checkpoint' verificada\")\n    \n   \n    years = range(2022, 2026) \n    months = range(1, 13)\n    services = ['yellow','green']\n    \n    all_combinations = []\n    for year, month, service in itertools.product(years, months, services):\n        year_month = f\"{year}-{month:02d}\"\n        all_combinations.append({\n            'year_month': year_month,\n            'service_type': service\n        })\n    \n    \n    with engine.connect() as conn:\n        completed = pd.read_sql(\n            \"\"\"\n            SELECT year_month, service_type \n            FROM orchestration.ingestion_checkpoint \n            WHERE status = 'completed'\n            \"\"\", \n            conn\n        )\n    \n    \n    completed_set = set(zip(completed['year_month'], completed['service_type']))\n    \n    \n    pending = []\n    for combo in all_combinations:\n        key = (combo['year_month'], combo['service_type'])\n        if key not in completed_set:\n            pending.append(combo)\n    \n    logger.info(f\"Total combinaciones: {len(all_combinations)}\")\n    logger.info(f\"Ya completadas: {len(completed_set)}\")\n    logger.info(f\"Pendientes: {len(pending)}\")\n    \n   \n    if pending:\n        logger.info(\"Primeras 5 pendientes:\")\n        for p in pending[:5]:\n            logger.info(f\"  - {p['service_type']} {p['year_month']}\")\n    \n    return pd.DataFrame(pending)\n\n@test\ndef test_output(output, *args) -> None:\n    assert output is not None, 'El output es undefined'", "file_path": "data_loaders/creating_checkpoint_scheme_table.py", "language": "python", "type": "data_loader", "uuid": "creating_checkpoint_scheme_table"}, "data_loaders/ing_taxi_trips.py:data_loader:python:ing taxi trips": {"content": "if 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test \n \nimport io\nimport pandas as pd\nimport requests\nfrom datetime import datetime\n\n\n@data_loader\ndef load_data(data, *args, **kwargs):\n    logger = kwargs.get('logger')\n    \"\"\"\n    Template code for loading data from any source.\n\n    Returns:\n        Anything (e.g. data frame, dictionary, array, int, str, etc.)\n    \"\"\"\n    year_month = kwargs.get(\"year_month\")\n    service_type = kwargs.get(\"service_type\")\n\n    current_date = datetime.now()\n    current_year_month = current_date.strftime(\"%Y-%m\")\n    \n    if not year_month:\n        year_month = current_year_month\n\n    if not service_type:\n        service_type = \"yellow\"\n\n\n    url_taxi_generic_link = \"https://d37ci6vzurychx.cloudfront.net/trip-data/\"\n    trips_url = url_taxi_generic_link + f\"{service_type}_tripdata_{year_month}.parquet\"\n\n\n    \n    try:\n        logger.info(f\"Iniciando descarga de {trips_url}\")\n        response = requests.get(trips_url)\n        response.raise_for_status()\n\n        trips_parquet = pd.read_parquet(io.BytesIO(response.content))\n        trips_parquet['ingest_ts'] = pd.Timestamp.now()\n        trips_parquet['source_month'] = year_month\n        trips_parquet['service_type'] = service_type \n        logger.info(\"Descarga exitosa\")\n    \n    except requests.exceptions.RequestException as e:\n        logger.error(f\"Error al descargar {trips_url}: {e}\")\n\n\n    column_list = trips_parquet.columns.tolist()\n    logger.info(f\"N\u00famero de columnas: {len(column_list)}\")\n    logger.info(f\"Nombres: {column_list}\")\n    return trips_parquet\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "data_loaders/ing_taxi_trips.py", "language": "python", "type": "data_loader", "uuid": "ing_taxi_trips"}, "data_loaders/load_titanic.py:data_loader:python:load titanic": {"content": "import io\nimport pandas as pd\nimport requests\nfrom pandas import DataFrame\n\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@data_loader\ndef load_data_from_api(**kwargs) -> DataFrame:\n    \"\"\"\n    Template for loading data from API\n    \"\"\"\n    url = 'https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv?raw=True'\n\n    return pd.read_csv(url)\n\n\n@test\ndef test_output(df) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert df is not None, 'The output is undefined'\n", "file_path": "data_loaders/load_titanic.py", "language": "python", "type": "data_loader", "uuid": "load_titanic"}, "data_loaders/load_zones.py:data_loader:python:load zones": {"content": "if 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\nimport pandas as pd\nfrom mage_ai.data_preparation.shared.secrets import get_secret_value\nfrom sqlalchemy import create_engine, text\nfrom datetime import datetime \n \n@data_loader\ndef load_data(*args, **kwargs):\n    logger = kwargs.get('logger')\n    \n    \n    year_month = kwargs.get(\"year_month\")\n    service_type = kwargs.get(\"service_type\")\n    \n    if not year_month:\n        year_month = \"2025-01\"\n    if not service_type:\n        service_type = \"green\"\n    \n    logger.info(f\"=== Procesando {service_type} - {year_month} ===\")\n    \n    \n    db_user = get_secret_value('POSTGRES_USER')\n    db_password = get_secret_value('POSTGRES_PASSWORD')\n    db_host = get_secret_value('POSTGRES_HOST')\n    db_port = '5432'\n    db_name = get_secret_value('POSTGRES_DB')\n    \n    connection_string = f'postgresql://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}'\n    engine = create_engine(connection_string)\n    \n    \n    with engine.connect() as conn:\n        with conn.begin():\n            \n            result = conn.execute(\n                text(\"\"\"\n                    SELECT status FROM orchestration.ingestion_checkpoint \n                    WHERE year_month = :month AND service_type = :service\n                \"\"\"),\n                {\"month\": year_month, \"service\": service_type}\n            ).first()\n            \n            if result:\n                \n                if result[0] != 'completed':\n                    conn.execute(\n                        text(\"\"\"\n                            UPDATE orchestration.ingestion_checkpoint \n                            SET status = 'started', \n                                started_at = NOW(),\n                                retry_count = retry_count + 1,\n                                updated_at = NOW()\n                            WHERE year_month = :month AND service_type = :service\n                        \"\"\"),\n                        {\"month\": year_month, \"service\": service_type}\n                    )\n                    logger.info(f\"\u2713 Checkpoint actualizado: {service_type} - {year_month} -> processing\")\n                else:\n                    logger.info(f\"\u26a0\ufe0f {service_type} - {year_month} ya est\u00e1 COMPLETADO\")\n            else:\n                \n                conn.execute(\n                    text(\"\"\"\n                        INSERT INTO orchestration.ingestion_checkpoint \n                        (year_month, service_type, status, started_at)\n                        VALUES (:month, :service, 'started', NOW())\n                    \"\"\"),\n                    {\"month\": year_month, \"service\": service_type}\n                )\n                logger.info(f\"\u2713 Nuevo checkpoint creado: {service_type} - {year_month}\")\n\n    \n    url_taxi_zones = 'https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv'\n\n    logger.info(' Inicio de descarga de zonas')\n    try:\n        zones_csv = pd.read_csv(url_taxi_zones)\n        logger.info(f\"Se descargaron las zonas exitosamente, con shape {zones_csv.shape}\")\n    except Exception as e:\n        logger.error(f\"Error en la descarga de zonas: {e}\")\n        \n        \n        with engine.connect() as conn:\n            with conn.begin():\n                conn.execute(\n                    text(\"\"\"\n                        UPDATE orchestration.ingestion_checkpoint \n                        SET status = 'failed', \n                            error_message = :error,\n                            updated_at = NOW()\n                        WHERE year_month = :month AND service_type = :service\n                    \"\"\"),\n                    {\n                        \"month\": year_month, \n                        \"service\": service_type,\n                        \"error\": str(e)[:200]\n                    }\n                )\n        raise e\n\n    return zones_csv\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'", "file_path": "data_loaders/load_zones.py", "language": "python", "type": "data_loader", "uuid": "load_zones"}, "dbts/run_tests_int_taxi_trips_zones_joined.yaml:dbt:yaml:run tests int taxi trips zones joined": {"content": "--select int_taxi_trips_zones_joined", "file_path": "dbts/run_tests_int_taxi_trips_zones_joined.yaml", "language": "yaml", "type": "dbt", "uuid": "run_tests_int_taxi_trips_zones_joined"}, "dbts/test_run_dim_date.yaml:dbt:yaml:test run dim date": {"content": "--select dim_date", "file_path": "dbts/test_run_dim_date.yaml", "language": "yaml", "type": "dbt", "uuid": "test_run_dim_date"}, "dbts/test_run_dim_payment_type.yaml:dbt:yaml:test run dim payment type": {"content": "--select dim_payment_type", "file_path": "dbts/test_run_dim_payment_type.yaml", "language": "yaml", "type": "dbt", "uuid": "test_run_dim_payment_type"}, "dbts/test_run_dim_service_type.yaml:dbt:yaml:test run dim service type": {"content": "--select dim_service_type", "file_path": "dbts/test_run_dim_service_type.yaml", "language": "yaml", "type": "dbt", "uuid": "test_run_dim_service_type"}, "dbts/test_run_dim_vendors.yaml:dbt:yaml:test run dim vendors": {"content": "--select dim_vendors", "file_path": "dbts/test_run_dim_vendors.yaml", "language": "yaml", "type": "dbt", "uuid": "test_run_dim_vendors"}, "dbts/test_run_dim_zones.yaml:dbt:yaml:test run dim zones": {"content": "--select dim_zones", "file_path": "dbts/test_run_dim_zones.yaml", "language": "yaml", "type": "dbt", "uuid": "test_run_dim_zones"}, "dbts/test_run_fct_trips.yaml:dbt:yaml:test run fct trips": {"content": "--select fct_trips", "file_path": "dbts/test_run_fct_trips.yaml", "language": "yaml", "type": "dbt", "uuid": "test_run_fct_trips"}, "transformers/fill_in_missing_values.py:transformer:python:fill in missing values": {"content": "from pandas import DataFrame\nimport math\n\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\ndef select_number_columns(df: DataFrame) -> DataFrame:\n    return df[['Age', 'Fare', 'Parch', 'Pclass', 'SibSp', 'Survived']]\n\n\ndef fill_missing_values_with_median(df: DataFrame) -> DataFrame:\n    for col in df.columns:\n        values = sorted(df[col].dropna().tolist())\n        median_value = values[math.floor(len(values) / 2)]\n        df[[col]] = df[[col]].fillna(median_value)\n    return df\n\n\n@transformer\ndef transform_df(df: DataFrame, *args, **kwargs) -> DataFrame:\n    \"\"\"\n    Template code for a transformer block.\n\n    Add more parameters to this function if this block has multiple parent blocks.\n    There should be one parameter for each output variable from each parent block.\n\n    Args:\n        df (DataFrame): Data frame from parent block.\n\n    Returns:\n        DataFrame: Transformed data frame\n    \"\"\"\n    # Specify your transformation logic here\n\n    return fill_missing_values_with_median(select_number_columns(df))\n\n\n@test\ndef test_output(df) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert df is not None, 'The output is undefined'\n", "file_path": "transformers/fill_in_missing_values.py", "language": "python", "type": "transformer", "uuid": "fill_in_missing_values"}, "pipelines/backfill/metadata.yaml:pipeline:yaml:backfill/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - executing_backfill\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: creating_checkpoint_scheme_table\n  retry_config: null\n  status: updated\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: creating_checkpoint_scheme_table\n- all_upstream_blocks_executed: false\n  color: null\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: executing_backfill\n  retry_config: null\n  status: updated\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - creating_checkpoint_scheme_table\n  uuid: executing_backfill\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2026-02-18 23:58:57.533952+00:00'\ndata_integration: null\ndescription: null\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: backfill\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: backfill\nvariables_dir: /home/src/mage_data/default_repo\nwidgets: []\n", "file_path": "pipelines/backfill/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "backfill/metadata"}, "pipelines/backfill/__init__.py:pipeline:python:backfill/  init  ": {"content": "", "file_path": "pipelines/backfill/__init__.py", "language": "python", "type": "pipeline", "uuid": "backfill/__init__"}, "pipelines/dbt_build_gold/metadata.yaml:pipeline:yaml:dbt build gold/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    dbt_project_name: dbt/scheduler_transformations\n    file_path: dbt/scheduler_transformations/models/gold/dim_payment_type.sql\n    file_source:\n      path: dbt/scheduler_transformations/models/gold/dim_payment_type.sql\n      project_path: dbt/scheduler_transformations\n  downstream_blocks:\n  - test_run_dim_payment_type\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: sql\n  name: dim_payment_type\n  retry_config: null\n  status: failed\n  timeout: null\n  type: dbt\n  upstream_blocks: []\n  uuid: dbt/scheduler_transformations/models/gold/dim_payment_type\n- all_upstream_blocks_executed: false\n  color: null\n  configuration:\n    dbt:\n      command: test\n    dbt_profile_target: null\n    dbt_project_name: dbt/scheduler_transformations\n    disable_query_preprocessing: false\n    export_write_policy: append\n    file_source:\n      path: dbts/test_run_dim_payment_type.yaml\n    use_raw_sql: false\n  downstream_blocks:\n  - dbt/scheduler_transformations/models/gold/dim_date\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: yaml\n  name: test_run_dim_payment_type\n  retry_config: null\n  status: failed\n  timeout: null\n  type: dbt\n  upstream_blocks:\n  - dbt/scheduler_transformations/models/gold/dim_payment_type\n  uuid: test_run_dim_payment_type\n- all_upstream_blocks_executed: false\n  color: null\n  configuration:\n    dbt_project_name: dbt/scheduler_transformations\n    file_path: dbt/scheduler_transformations/models/gold/dim_date.sql\n    file_source:\n      path: dbt/scheduler_transformations/models/gold/dim_date.sql\n      project_path: dbt/scheduler_transformations\n  downstream_blocks:\n  - test_run_dim_date\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: sql\n  name: dim_date\n  retry_config: null\n  status: failed\n  timeout: null\n  type: dbt\n  upstream_blocks:\n  - test_run_dim_payment_type\n  uuid: dbt/scheduler_transformations/models/gold/dim_date\n- all_upstream_blocks_executed: false\n  color: null\n  configuration:\n    dbt:\n      command: test\n    dbt_profile_target: ''\n    dbt_project_name: dbt/scheduler_transformations\n    disable_query_preprocessing: false\n    export_write_policy: append\n    file_source:\n      path: dbts/test_run_dim_date.yaml\n    use_raw_sql: false\n  downstream_blocks:\n  - dbt/scheduler_transformations/models/gold/dim_service_type\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: yaml\n  name: test_run_dim_date\n  retry_config: null\n  status: failed\n  timeout: null\n  type: dbt\n  upstream_blocks:\n  - dbt/scheduler_transformations/models/gold/dim_date\n  uuid: test_run_dim_date\n- all_upstream_blocks_executed: false\n  color: null\n  configuration:\n    dbt_project_name: dbt/scheduler_transformations\n    file_path: dbt/scheduler_transformations/models/gold/dim_service_type.sql\n    file_source:\n      path: dbt/scheduler_transformations/models/gold/dim_service_type.sql\n      project_path: dbt/scheduler_transformations\n  downstream_blocks:\n  - test_run_dim_service_type\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: sql\n  name: dim_service_type\n  retry_config: null\n  status: failed\n  timeout: null\n  type: dbt\n  upstream_blocks:\n  - test_run_dim_date\n  uuid: dbt/scheduler_transformations/models/gold/dim_service_type\n- all_upstream_blocks_executed: false\n  color: null\n  configuration:\n    dbt:\n      command: test\n    dbt_profile_target: ''\n    dbt_project_name: dbt/scheduler_transformations\n    disable_query_preprocessing: false\n    export_write_policy: append\n    file_source:\n      path: dbts/test_run_dim_service_type.yaml\n    use_raw_sql: false\n  downstream_blocks:\n  - dbt/scheduler_transformations/models/gold/dim_zones\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: yaml\n  name: test_run_dim_service_type\n  retry_config: null\n  status: updated\n  timeout: null\n  type: dbt\n  upstream_blocks:\n  - dbt/scheduler_transformations/models/gold/dim_service_type\n  uuid: test_run_dim_service_type\n- all_upstream_blocks_executed: false\n  color: null\n  configuration:\n    dbt_project_name: dbt/scheduler_transformations\n    file_path: dbt/scheduler_transformations/models/gold/dim_zones.sql\n    file_source:\n      path: dbt/scheduler_transformations/models/gold/dim_zones.sql\n      project_path: dbt/scheduler_transformations\n  downstream_blocks:\n  - test_run_dim_zones\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: sql\n  name: dim_zones\n  retry_config: null\n  status: failed\n  timeout: null\n  type: dbt\n  upstream_blocks:\n  - test_run_dim_service_type\n  uuid: dbt/scheduler_transformations/models/gold/dim_zones\n- all_upstream_blocks_executed: false\n  color: null\n  configuration:\n    dbt:\n      command: test\n    dbt_profile_target: ''\n    dbt_project_name: dbt/scheduler_transformations\n    disable_query_preprocessing: false\n    export_write_policy: append\n    file_source:\n      path: dbts/test_run_dim_zones.yaml\n    use_raw_sql: false\n  downstream_blocks:\n  - dbt/scheduler_transformations/models/gold/dim_vendors\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: yaml\n  name: test_run_dim_zones\n  retry_config: null\n  status: updated\n  timeout: null\n  type: dbt\n  upstream_blocks:\n  - dbt/scheduler_transformations/models/gold/dim_zones\n  uuid: test_run_dim_zones\n- all_upstream_blocks_executed: false\n  color: null\n  configuration:\n    dbt_project_name: dbt/scheduler_transformations\n    file_path: dbt/scheduler_transformations/models/gold/dim_vendors.sql\n    file_source:\n      path: dbt/scheduler_transformations/models/gold/dim_vendors.sql\n      project_path: dbt/scheduler_transformations\n  downstream_blocks:\n  - test_run_dim_vendors\n  - dbt/scheduler_transformations/models/gold/fct_trips\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: sql\n  name: dim_vendors\n  retry_config: null\n  status: failed\n  timeout: null\n  type: dbt\n  upstream_blocks:\n  - test_run_dim_zones\n  uuid: dbt/scheduler_transformations/models/gold/dim_vendors\n- all_upstream_blocks_executed: false\n  color: null\n  configuration:\n    dbt:\n      command: test\n    dbt_profile_target: ''\n    dbt_project_name: dbt/scheduler_transformations\n    disable_query_preprocessing: false\n    export_write_policy: append\n    file_source:\n      path: dbts/test_run_dim_vendors.yaml\n    use_raw_sql: false\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: yaml\n  name: test_run_dim_vendors\n  retry_config: null\n  status: updated\n  timeout: null\n  type: dbt\n  upstream_blocks:\n  - dbt/scheduler_transformations/models/gold/dim_vendors\n  uuid: test_run_dim_vendors\n- all_upstream_blocks_executed: false\n  color: null\n  configuration:\n    dbt_project_name: dbt/scheduler_transformations\n    file_path: dbt/scheduler_transformations/models/gold/fct_trips.sql\n    file_source:\n      path: dbt/scheduler_transformations/models/gold/fct_trips.sql\n      project_path: dbt/scheduler_transformations\n    limit: 1000\n  downstream_blocks:\n  - test_run_fct_trips\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: sql\n  name: dbt/scheduler_transformations/models/gold/fct_trips\n  retry_config: null\n  status: failed\n  timeout: null\n  type: dbt\n  upstream_blocks:\n  - dbt/scheduler_transformations/models/gold/dim_vendors\n  uuid: dbt/scheduler_transformations/models/gold/fct_trips\n- all_upstream_blocks_executed: false\n  color: null\n  configuration:\n    dbt:\n      command: test\n    disable_query_preprocessing: false\n    export_write_policy: append\n    file_source:\n      path: dbts/test_run_fct_trips.yaml\n    use_raw_sql: false\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: yaml\n  name: test_run_fct_trips\n  retry_config: null\n  status: updated\n  timeout: null\n  type: dbt\n  upstream_blocks:\n  - dbt/scheduler_transformations/models/gold/fct_trips\n  uuid: test_run_fct_trips\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2026-02-21 23:10:21.203209+00:00'\ndata_integration: null\ndescription: null\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: dbt_build_gold\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: dbt_build_gold\nvariables_dir: /home/src/mage_data/default_repo\nwidgets: []\n", "file_path": "pipelines/dbt_build_gold/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "dbt_build_gold/metadata"}, "pipelines/dbt_build_gold/__init__.py:pipeline:python:dbt build gold/  init  ": {"content": "", "file_path": "pipelines/dbt_build_gold/__init__.py", "language": "python", "type": "pipeline", "uuid": "dbt_build_gold/__init__"}, "pipelines/dbt_build_silver/metadata.yaml:pipeline:yaml:dbt build silver/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    dbt_project_name: dbt/scheduler_transformations\n    file_path: dbt/scheduler_transformations/models/silver/int_taxi_trips_zones_joined.sql\n    file_source:\n      path: dbt/scheduler_transformations/models/silver/int_taxi_trips_zones_joined.sql\n      project_path: dbt/scheduler_transformations\n    limit: 1000\n  downstream_blocks:\n  - run_tests_int_taxi_trips_zones_joined\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: sql\n  name: dbt/scheduler_transformations/models/silver/int_taxi_trips_zones_joined\n  retry_config: null\n  status: updated\n  timeout: null\n  type: dbt\n  upstream_blocks: []\n  uuid: dbt/scheduler_transformations/models/silver/int_taxi_trips_zones_joined\n- all_upstream_blocks_executed: false\n  color: null\n  configuration:\n    dbt:\n      command: test\n    dbt_profile_target: ''\n    dbt_project_name: dbt/scheduler_transformations\n    disable_query_preprocessing: false\n    export_write_policy: append\n    file_source:\n      path: dbts/run_tests_int_taxi_trips_zones_joined.yaml\n    use_raw_sql: false\n  downstream_blocks:\n  - start_dbt_build_gold\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: yaml\n  name: run_tests_int_taxi_trips_zones_joined\n  retry_config: null\n  status: updated\n  timeout: null\n  type: dbt\n  upstream_blocks:\n  - dbt/scheduler_transformations/models/silver/int_taxi_trips_zones_joined\n  uuid: run_tests_int_taxi_trips_zones_joined\n- all_upstream_blocks_executed: false\n  color: grey\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: start_dbt_build_gold\n  retry_config: null\n  status: executed\n  timeout: null\n  type: custom\n  upstream_blocks:\n  - run_tests_int_taxi_trips_zones_joined\n  uuid: start_dbt_build_gold\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2026-02-21 15:54:25.227900+00:00'\ndata_integration: null\ndescription: null\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: dbt_build_silver\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: dbt_build_silver\nvariables_dir: /home/src/mage_data/default_repo\nwidgets: []\n", "file_path": "pipelines/dbt_build_silver/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "dbt_build_silver/metadata"}, "pipelines/dbt_build_silver/__init__.py:pipeline:python:dbt build silver/  init  ": {"content": "", "file_path": "pipelines/dbt_build_silver/__init__.py", "language": "python", "type": "pipeline", "uuid": "dbt_build_silver/__init__"}, "pipelines/ingest_bronze/metadata.yaml:pipeline:yaml:ingest bronze/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - uploading_zones_to_db\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: load_zones\n  retry_config: null\n  status: updated\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: load_zones\n- all_upstream_blocks_executed: false\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - dbt/scheduler_transformations/models/bronze/stg_zones\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: uploading_zones_to_db\n  retry_config: null\n  status: updated\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - load_zones\n  uuid: uploading_zones_to_db\n- all_upstream_blocks_executed: false\n  color: null\n  configuration:\n    dbt_project_name: dbt/scheduler_transformations\n    file_path: dbt/scheduler_transformations/models/bronze/stg_zones.sql\n    file_source:\n      path: dbt/scheduler_transformations/models/bronze/stg_zones.sql\n      project_path: dbt/scheduler_transformations\n    limit: 1000\n  downstream_blocks:\n  - ing_taxi_trips\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: sql\n  name: dbt/scheduler_transformations/models/bronze/stg_zones\n  retry_config: null\n  status: executed\n  timeout: null\n  type: dbt\n  upstream_blocks:\n  - uploading_zones_to_db\n  uuid: dbt/scheduler_transformations/models/bronze/stg_zones\n- all_upstream_blocks_executed: false\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - uploading_trips_to_db\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: ing_taxi_trips\n  retry_config: null\n  status: updated\n  timeout: null\n  type: data_loader\n  upstream_blocks:\n  - dbt/scheduler_transformations/models/bronze/stg_zones\n  uuid: ing_taxi_trips\n- all_upstream_blocks_executed: false\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - dbt/scheduler_transformations/models/bronze/stg_taxi_trips_bronze\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: uploading_trips_to_db\n  retry_config: null\n  status: updated\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - ing_taxi_trips\n  uuid: uploading_trips_to_db\n- all_upstream_blocks_executed: false\n  color: null\n  configuration:\n    dbt_project_name: dbt/scheduler_transformations\n    file_path: dbt/scheduler_transformations/models/bronze/stg_taxi_trips_bronze.sql\n    file_source:\n      path: dbt/scheduler_transformations/models/bronze/stg_taxi_trips_bronze.sql\n      project_path: dbt/scheduler_transformations\n  downstream_blocks:\n  - run_silver_pipeline\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: sql\n  name: stg_taxi_trips_bronze\n  retry_config: null\n  status: executed\n  timeout: null\n  type: dbt\n  upstream_blocks:\n  - uploading_trips_to_db\n  uuid: dbt/scheduler_transformations/models/bronze/stg_taxi_trips_bronze\n- all_upstream_blocks_executed: false\n  color: grey\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: run_silver_pipeline\n  retry_config: null\n  status: executed\n  timeout: null\n  type: custom\n  upstream_blocks:\n  - dbt/scheduler_transformations/models/bronze/stg_taxi_trips_bronze\n  uuid: run_silver_pipeline\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2026-02-18 21:00:13.130489+00:00'\ndata_integration: null\ndescription: null\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: ingest_bronze\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: ingest_bronze\nvariables_dir: /home/src/mage_data/default_repo\nwidgets: []\n", "file_path": "pipelines/ingest_bronze/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "ingest_bronze/metadata"}, "pipelines/ingest_bronze/__init__.py:pipeline:python:ingest bronze/  init  ": {"content": "", "file_path": "pipelines/ingest_bronze/__init__.py", "language": "python", "type": "pipeline", "uuid": "ingest_bronze/__init__"}}, "custom_block_template": {}, "mage_template": {"data_loaders/airtable.py:data_loader:python:Airtable:Load a Table from Airtable App.": {"block_type": "data_loader", "description": "Load a Table from Airtable App.", "language": "python", "name": "Airtable", "path": "data_loaders/airtable.py"}, "data_loaders/deltalake/s3.py:data_loader:python:Amazon S3:Load a Delta Table from Amazon S3.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_loaders/deltalake/s3.py"}, "data_loaders/deltalake/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Load a Delta Table from Azure Blob Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/deltalake/azure_blob_storage.py"}, "data_loaders/deltalake/gcs.py:data_loader:python:Google Cloud Storage:Load a Delta Table from Google Cloud Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/deltalake/gcs.py"}, "data_loaders/mongodb.py:data_loader:python:MongoDB:Load data from MongoDB.:Databases (NoSQL)": {"block_type": "data_loader", "description": "Load data from MongoDB.", "groups": ["Databases (NoSQL)"], "language": "python", "name": "MongoDB", "path": "data_loaders/mongodb.py"}, "data_loaders/mssql.py:data_loader:python:MSSQL:Load data from MSSQL.:Databases": {"block_type": "data_loader", "description": "Load data from MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_loaders/mssql.py"}, "data_exporters/deltalake/s3.py:data_exporter:python:Amazon S3:Export data to a Delta Table in Amazon S3.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_exporters/deltalake/s3.py"}, "data_exporters/deltalake/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Export data to a Delta Table in Azure Blob Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/deltalake/azure_blob_storage.py"}, "data_exporters/deltalake/gcs.py:data_exporter:python:Google Cloud Storage:Export data to a Delta Table in Google Cloud Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/deltalake/gcs.py"}, "data_exporters/mongodb.py:data_exporter:python:MongoDB:Export data to MongoDB.": {"block_type": "data_exporter", "description": "Export data to MongoDB.", "language": "python", "name": "MongoDB", "path": "data_exporters/mongodb.py"}, "data_exporters/mssql.py:data_exporter:python:MSSQL:Export data to MSSQL.:Databases": {"block_type": "data_exporter", "description": "Export data to MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_exporters/mssql.py"}, "data_loaders/orchestration/triggers/default.jinja:data_loader:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_loader", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_loaders/orchestration/triggers/default.jinja"}, "data_exporters/orchestration/triggers/default.jinja:data_exporter:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_exporter", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_exporters/orchestration/triggers/default.jinja"}, "callbacks/base.jinja:callback:python:Base template:Base template with empty functions.": {"block_type": "callback", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "callbacks/base.jinja"}, "callbacks/orchestration/triggers/default.jinja:callback:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "callback", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "callbacks/orchestration/triggers/default.jinja"}, "conditionals/base.jinja:conditional:python:Base template:Base template with empty functions.": {"block_type": "conditional", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "conditionals/base.jinja"}, "data_loaders/default.jinja:data_loader:python:Base template (generic)": {"block_type": "data_loader", "language": "python", "name": "Base template (generic)", "path": "data_loaders/default.jinja"}, "data_loaders/s3.py:data_loader:python:Amazon S3:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_loaders/s3.py"}, "data_loaders/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/azure_blob_storage.py"}, "data_loaders/google_cloud_storage.py:data_loader:python:Google Cloud Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/google_cloud_storage.py"}, "data_loaders/redshift.py:data_loader:python:Amazon Redshift:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_loaders/redshift.py"}, "data_loaders/bigquery.py:data_loader:python:Google BigQuery:Load data from Google BigQuery.:Data warehouses": {"block_type": "data_loader", "description": "Load data from Google BigQuery.", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_loaders/bigquery.py"}, "data_loaders/snowflake.py:data_loader:python:Snowflake:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_loaders/snowflake.py"}, "data_loaders/algolia.py:data_loader:python:Algolia:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_loaders/algolia.py"}, "data_loaders/chroma.py:data_loader:python:Chroma:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_loaders/chroma.py"}, "data_loaders/duckdb.py:data_loader:python:DuckDB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_loaders/duckdb.py"}, "data_loaders/mysql.py:data_loader:python:MySQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_loaders/mysql.py"}, "data_loaders/oracledb.py:data_loader:python:Oracle DB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Oracle DB", "path": "data_loaders/oracledb.py"}, "data_loaders/postgres.py:data_loader:python:PostgreSQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_loaders/postgres.py"}, "data_loaders/qdrant.py:data_loader:python:Qdrant:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_loaders/qdrant.py"}, "data_loaders/weaviate.py:data_loader:python:Weaviate:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_loaders/weaviate.py"}, "data_loaders/api.py:data_loader:python:API:Fetch data from an API request.": {"block_type": "data_loader", "description": "Fetch data from an API request.", "language": "python", "name": "API", "path": "data_loaders/api.py"}, "data_loaders/file.py:data_loader:python:Local file:Load data from a file on your machine.": {"block_type": "data_loader", "description": "Load data from a file on your machine.", "language": "python", "name": "Local file", "path": "data_loaders/file.py"}, "data_loaders/google_sheets.py:data_loader:python:Google Sheets:Load data from a worksheet in Google Sheets.": {"block_type": "data_loader", "description": "Load data from a worksheet in Google Sheets.", "language": "python", "name": "Google Sheets", "path": "data_loaders/google_sheets.py"}, "data_loaders/druid.py:data_loader:python:Druid": {"block_type": "data_loader", "language": "python", "name": "Druid", "path": "data_loaders/druid.py"}, "transformers/default.jinja:transformer:python:Base template (generic)": {"block_type": "transformer", "language": "python", "name": "Base template (generic)", "path": "transformers/default.jinja"}, "transformers/data_warehouse_transformer.jinja:transformer:python:Amazon Redshift:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "redshift", "data_source_handler": "Redshift"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Google BigQuery:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "", "data_source": "bigquery", "data_source_handler": "BigQuery"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Snowflake:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "snowflake", "data_source_handler": "Snowflake"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:PostgreSQL:Databases": {"block_type": "transformer", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "postgres", "data_source_handler": "Postgres"}}, "transformers/transformer_actions/row/drop_duplicate.py:transformer:python:Drop duplicate rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Drop duplicate rows", "path": "transformers/transformer_actions/row/drop_duplicate.py"}, "transformers/transformer_actions/row/filter.py:transformer:python:Filter rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Filter rows", "path": "transformers/transformer_actions/row/filter.py"}, "transformers/transformer_actions/row/remove.py:transformer:python:Remove rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Remove rows", "path": "transformers/transformer_actions/row/remove.py"}, "transformers/transformer_actions/row/sort.py:transformer:python:Sort rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Sort rows", "path": "transformers/transformer_actions/row/sort.py"}, "transformers/transformer_actions/column/average.py:transformer:python:Average value of column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Average value of column", "path": "transformers/transformer_actions/column/average.py"}, "transformers/transformer_actions/column/count_distinct.py:transformer:python:Count unique values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Count unique values in column", "path": "transformers/transformer_actions/column/count_distinct.py"}, "transformers/transformer_actions/column/first.py:transformer:python:First value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "First value in column", "path": "transformers/transformer_actions/column/first.py"}, "transformers/transformer_actions/column/last.py:transformer:python:Last value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Last value in column", "path": "transformers/transformer_actions/column/last.py"}, "transformers/transformer_actions/column/max.py:transformer:python:Maximum value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Maximum value in column", "path": "transformers/transformer_actions/column/max.py"}, "transformers/transformer_actions/column/median.py:transformer:python:Median value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Median value in column", "path": "transformers/transformer_actions/column/median.py"}, "transformers/transformer_actions/column/min.py:transformer:python:Min value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Min value in column", "path": "transformers/transformer_actions/column/min.py"}, "transformers/transformer_actions/column/sum.py:transformer:python:Sum of all values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Sum of all values in column", "path": "transformers/transformer_actions/column/sum.py"}, "transformers/transformer_actions/column/count.py:transformer:python:Total count of values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Total count of values in column", "path": "transformers/transformer_actions/column/count.py"}, "transformers/transformer_actions/column/clean_column_name.py:transformer:python:Clean column name:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Clean column name", "path": "transformers/transformer_actions/column/clean_column_name.py"}, "transformers/transformer_actions/column/fix_syntax_errors.py:transformer:python:Fix syntax errors:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Fix syntax errors", "path": "transformers/transformer_actions/column/fix_syntax_errors.py"}, "transformers/transformer_actions/column/reformat.py:transformer:python:Reformat values in column:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Reformat values in column", "path": "transformers/transformer_actions/column/reformat.py"}, "transformers/transformer_actions/column/select.py:transformer:python:Keep column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Keep column(s)", "path": "transformers/transformer_actions/column/select.py"}, "transformers/transformer_actions/column/remove.py:transformer:python:Remove column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Remove column(s)", "path": "transformers/transformer_actions/column/remove.py"}, "transformers/transformer_actions/column/shift_down.py:transformer:python:Shift row values down:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values down", "path": "transformers/transformer_actions/column/shift_down.py"}, "transformers/transformer_actions/column/shift_up.py:transformer:python:Shift row values up:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values up", "path": "transformers/transformer_actions/column/shift_up.py"}, "transformers/transformer_actions/column/normalize.py:transformer:python:Normalize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Normalize data", "path": "transformers/transformer_actions/column/normalize.py"}, "transformers/transformer_actions/column/standardize.py:transformer:python:Standardize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Standardize data", "path": "transformers/transformer_actions/column/standardize.py"}, "transformers/transformer_actions/column/impute.py:transformer:python:Fill in missing values:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Fill in missing values", "path": "transformers/transformer_actions/column/impute.py"}, "transformers/transformer_actions/column/remove_outliers.py:transformer:python:Remove outliers:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Remove outliers", "path": "transformers/transformer_actions/column/remove_outliers.py"}, "transformers/transformer_actions/column/diff.py:transformer:python:Calculate difference between values:Column actions:Feature extraction": {"block_type": "transformer", "groups": ["Column actions", "Feature extraction"], "language": "python", "name": "Calculate difference between values", "path": "transformers/transformer_actions/column/diff.py"}, "data_exporters/default.jinja:data_exporter:python:Base template (generic)": {"block_type": "data_exporter", "language": "python", "name": "Base template (generic)", "path": "data_exporters/default.jinja"}, "data_exporters/file.py:data_exporter:python:Local file": {"block_type": "data_exporter", "language": "python", "name": "Local file", "path": "data_exporters/file.py"}, "data_exporters/google_sheets.py:data_exporter:python:Google Sheets": {"block_type": "data_exporter", "language": "python", "name": "Google Sheets", "path": "data_exporters/google_sheets.py"}, "data_exporters/s3.py:data_exporter:python:Amazon S3:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_exporters/s3.py"}, "data_exporters/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/azure_blob_storage.py"}, "data_exporters/google_cloud_storage.py:data_exporter:python:Google Cloud Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/google_cloud_storage.py"}, "data_exporters/redshift.py:data_exporter:python:Amazon Redshift:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_exporters/redshift.py"}, "data_exporters/bigquery.py:data_exporter:python:Google BigQuery:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_exporters/bigquery.py"}, "data_exporters/snowflake.py:data_exporter:python:Snowflake:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_exporters/snowflake.py"}, "data_exporters/algolia.py:data_exporter:python:Algolia:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_exporters/algolia.py"}, "data_exporters/chroma.py:data_exporter:python:Chroma:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_exporters/chroma.py"}, "data_exporters/duckdb.py:data_exporter:python:DuckDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_exporters/duckdb.py"}, "data_exporters/mysql.py:data_exporter:python:MySQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_exporters/mysql.py"}, "data_exporters/oracledb.py:data_exporter:python:OracleDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "OracleDB", "path": "data_exporters/oracledb.py"}, "data_exporters/postgres.py:data_exporter:python:PostgreSQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_exporters/postgres.py"}, "data_exporters/qdrant.py:data_exporter:python:Qdrant:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_exporters/qdrant.py"}, "data_exporters/weaviate.py:data_exporter:python:Weaviate:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_exporters/weaviate.py"}, "sensors/default.py:sensor:python:Base template (generic)": {"block_type": "sensor", "language": "python", "name": "Base template (generic)", "path": "sensors/default.py"}, "sensors/s3.py:sensor:python:Amazon S3:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "sensors/s3.py"}, "sensors/google_cloud_storage.py:sensor:python:Google Cloud Storage:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "sensors/google_cloud_storage.py"}, "sensors/redshift.py:sensor:python:Amazon Redshift:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "sensors/redshift.py"}, "sensors/bigquery.py:sensor:python:Google BigQuery:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "sensors/bigquery.py"}, "sensors/snowflake.py:sensor:python:Snowflake:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "sensors/snowflake.py"}, "sensors/mysql.py:sensor:python:MySQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "sensors/mysql.py"}, "sensors/postgres.py:sensor:python:PostgreSQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "sensors/postgres.py"}}}